
# Hola, soy Mauro

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Perfil-blue)](https://www.linkedin.com/in/mauro-rivera-salinas/)
[![Portfolio](https://img.shields.io/badge/Portfolio-Web-lightgrey)](https://tu-url)
[![CV](https://img.shields.io/badge/CV-PDF-green)](https://github.com/maaurors/CV/blob/main/docs/Mauro_CV.pdf)
[![Email](https://img.shields.io/badge/Email-contacto-red)](maauro.rs@gmail.com)

Ingeniero en Informática | Data Engineer | GCP-first  
ETL/ELT con Python | Airflow | BigQuery | Dataflow | Cloud Run/Functions  
Migración Pentaho→Python | Integraciones API | SFTP | Buenas prácticas de datos  
Interés en ciberseguridad ofensiva ética y hardening de pipelines

---

## Tech stack
- Lenguajes: Python, SQL, Bash
- GCP: BigQuery, Dataflow, Cloud Run, Cloud Functions, Pub/Sub, Secret Manager, IAM
- Orquestación: Airflow, Composer
- Datos: pandas, pyarrow, BigQuery Storage API
- DevOps: Docker, GitLab CI, Terraform (básico)
- Otros: KNIME, Pentaho, Vertex AI (RAG/Chatbots)

---

## Lo que hago
- Diseño y orquesto pipelines robustos de ingesta, transformación y entrega en GCP.
- Migro jobs legados de Pentaho a Python contenedorizado ejecutando en Cloud Run.
- Optimizo costos y rendimiento en BigQuery (particiones, clustering, jobs batch).
- Construyo conectores API→BQ y exportadores BQ→SFTP con logging y trazabilidad.
- Creo DAGs de Airflow auditables y reproducibles.

---

## Proyectos destacados
- **airflow-dag-template-gcp**: plantilla productiva de DAG con sensores, retries, SLA y alertas.
- **pentaho-to-python-cloudrun**: boilerplate para portar transformaciones a Python + Docker + Cloud Run.
- **dataflow-bq-lakehouse**: pipeline de ingesta escalable (stream/batch) hacia BigQuery particionado.
- **bq-storageapi-loader**: carga masiva con BigQuery Storage API y control de esquemas.
- **sftp-exporter**: exportación de resultados a SFTP con control de nombres por período y logs.
- **zeroq-reservas-etl-demo**: demo de integración API→CSV→BQ parametrizable por rango de fechas.
- **vertex-rag-gcs**: chatbot interno con documentos en GCS y control de seguridad.
- **risk-model-framework**: esqueleto versionado para modelos de riesgo reutilizables en una caja de compensación.

> Publica cada repo con: README técnico, diagrama simple, `Makefile`/`invoke`, variables por `.env`, tests mínimos y dataset sintético.

---

## Métricas
[![GitHub Streak](https://streak-stats.demolab.com?user=maaurors&hide_border=true&border_radius=6&locale=es&short_numbers=true&date_format=M%20j%5B%2C%20Y%5D)](https://git.io/streak-stats)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=anuraghazra&layout=compact)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs?username=maauros&layout=compact&theme=dark&hide_border=true&langs_count=6&hide=javascript,glsl,c%2B%2B&custom_title=Lenguajes%20m%C3%A1s%20usados&size_weight=0.4&count_weight=0.6&v=2)


---

## Publicaciones y notas
- Pentaho→Python en GCP: decisiones, anti-patrones y playbook de migración.
- Cost-aware BigQuery: partición vs clustering y slots vs on-demand.
- Airflow en producción: idempotencia, reintentos y data lineage útil.

---

## Formación
- Cursos/bootcamps: Python (Desafío Latam), Airflow, Dataflow, Seguridad Ofensiva básica.
- Cursando Ingeieria en Informática.
- Diplomado Big Data & Ciencia de Datos.
- Lecturas técnicas y laboratorios propios publicados en `docs/` de cada repo.

---

## Contacto
LinkedIn, email y disponibilidad para colaborar en proyectos de datos y automatización.
